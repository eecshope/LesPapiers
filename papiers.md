---
layout: page
title: Les Papiers
subtitle: ......
---
# Les Papiers
Ces sont les papiers de notre groupe. Lisez-les s'il vous plait!

## Octobre 2019
### 2019/10/30
- *Novel positional encodings to enable tree-based transformers*, Yongmin Li
### 2019/12/25
- *Tree-Structured Attention with Hierarchical Accumulation*, Yongmin Li
### 2020/04/01
- *Mogrifier LSTM*, ICLR 2020, Yongmin Li
### 2020/07/15
- *BPE-dropout: Simple and Effective Subword Regularization*, ACL 2020, Yongmin Li
### 2020/08/19
- *Few-Shot Representation Learning for Out-Of-Vocabulary Words*, ACL 2019, Yongmin Li

##  November 2020
### 2020/11/18
- *Semantic Evaluation for Text-to-SQL with Distilled Test Suites*, EMNLP 2020, Yunfei Zhao
### 2020/11/25
- *Retrieval-Augmented Generation for Code Summarization via Hybrid GNN*, ICLR 2021 under review, Yongmin Li

##  April 2021
### 2021/04/28
- *Finetuning Pretrained Transformers into RNNs*, Yongmin Li

## June 2021
### 2021/06/21
- *Learning to Generate Code Sketches*, Yongmin Li

## July 2021
### 2021/07/07
- *Exploring Dynamic Selection of Branch Expansion Orders for Code Generation*, ACL 2021, Yongmin Li

## August 2021
### 2021/08/25
- *Why My Code Summarization Model Does Not Work: Code Comment Improvement with Category Prediction*, TOSEM 2021, Yongmin Li
- *Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs*, ACL 2020, Yunfei Zhao

## October 2021
### 2021/10/20
- *CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation*, Yongmin Li

## December 2021
### 2021/12/22
- *PLUR: A Unifying, Graph-Based View of Program Learning, Understanding, and Repair*, NIPS 2021 Spotlight, Yongmin Li

## February 2022
### 2022/02/23
- *Compositional Generalization via Semantic Tagging*, EMNLP 2021 Findings, Yongmin Li

## March 2020
### 2020/03/22
- *Code Search based on Context-aware Code Translation*, ICSE 2022, Yunfei Zhao

## May 2020
### 2022/05/11
- *FIRA: Fine-Grained Graph-Based Code Change Representation for Automated Commit Message Generation*, ICSE 2022, Yongmin Li

## June 2022
### 2022/06/01
- *Memorizing Transformers*, ICLR 2022, Yunfei Zhao
### 2022/06/22
- *CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation*, IJCAI 2022, Yongmin Li
- *Target-Side Input Augmentation for Sequence to Sequence Generation*, ICLR 2022, Yongmin Li

## July 2022
### 2022/07/13
- *Cross-Lingual Adaptation for Type Inference*, ISSTA 2022, Yunfei Zhao

## August 2022
### 2022/08/03
- *Directed Acyclic Transformer for Non-Autoregressive Machine Translation*, ICML 2022, Yongmin Li
### 2022/08/17
- *Diet Code is Healthy: Simplifying Programs for Pre-trained Models of Code*, FSE 2022, Yunfei Zhao
- *What do pre-trained code models know about code?*, ASE(NIER) 2021, Yunfei Zhao

## September 2022
### 2022/09/19
- *AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models*, ASE 2022, Yongmin Li
### 2022/09/26
- *Natural Test Generation for Precise Testing of Question Answering Software*, ASE 2022, Yunfei Zhao

## October 2022
### 2022/10/31
- *NS^3: Neuro-Symbolic Semantic Code Search*, NIPS 2022, Yongmin Li

## November 2022
### 2022/11/14
- *CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search*, EMNLP 2022, Yunfei Zhao

## Decembre 2022
### Decembre 12
- *No More Fine-tuning? An Experimental Evluation of Prompt Tuning in Code Intelligence*, Wang et al., FSE 2022, Jia Li
- *Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code*, Bareiβ, arXiv, Jia Li
- *DocPrompting: Genenrating Code By Retrieving the Docs*, Zhou et al., arXiv 2207.05987, Yongmin Li
- *COMET: A Neural Framework for MT Evaluation*, Rei et al., EMNLP 2020, Yihong Dong
- *UniTE: Unified Translation Evaluation*, Wan et al., ACL (1) 2022, Yihong Dong
### Decembre 19
- *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*, Wei et al., Nips 2022, Xue Jiang
- *Least-to-Most Prompting Enables Complex Reasoning in Large Language Models*, Zhou et al., ICLR 2023 **Under Review**, Xue Jiang
- *PAL: Program-aided Language Models*, Gao et al., arXiv, Xue Jiang
- *XPrompt: Exploring the Extreme of Prompt Tuning*, Ma et al., EMNLP 2022, Han Peng
- *An Explanation of In-Context learning as Implicit Bayesian Inference*, Xie et al., ICLR 2022, Zejun Wang
### Decembre 29
- *A Contrastive Framework for Neural Text Generation*, Su et al., Nips 2022, Yunfei Zhao
- *SAS: Self-Augmentation Strategy for Langauge Model Pre-training*, Xu et al., AAAI 2022, Haojie Zhang
- *Learning to Retrieve Prompts for In-Context Learning*, Rubin et all, NAACL 2022, Kechi Zhang
## Janvier 2023
### Janvier 5
- *ReCode: Robustness Evaluation of Code Generation Models*, Wang et al., arXiv, Yuqi Zhu
- *Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers*, Dai et al., arXiv, Jiaa Li
- *Self-Instruct: Aligning Language Model with Self Generated Instructions*, Wang et al., arXiv, Zhuo Li
## Février 2023
### Février 2
- *Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks*, Wechu Chen et al., ?, Zheng Fang 
- *Adaptive Performance Anomaly Detection for Online Service Systems via Pattern Sketching*, ?, ICSE 2022, Zhen Yang
- *Explaining Software Bugs Leveraging Code Structures in Neural Maching Translation*, Mahbub et al, ICSE 2023, Mingyang Geng

## Février 6
- *CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation*, Dong et al., ISSTA 2023, Yihong Dong
- *Large Langauge Models are reasoners with Self-Verification*, Weng et al., ?, Xue Jiang

## Février 13
- *CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models*, Yu et al., ?, Yongmin Li
- *Soft-labeled Contrastive Pre-training for Fucntion-level Code Representation*, Li et al., EMNLP 2022, Han Peng
- *Tailoring Langauge Generation Models under Total Variation Distance*, Ji et al., ICLR 2023, Zejun Wang
- *Encoding Recurrence into Transformers*, Huang et al., ICLR 2023, Zejun Wang
- *CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context*, Ding et al., ?, Yunfei Zhao

## Février 20
- *Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing*, Dai et al., Nips 2020, Haojie Zhang
- *Toolformer: Language Models Can Teach Themselves to Use Tools*, Schick et al., ?, Kechi Zhang
- *Gradient-Based Constrained Sampling from Language Models*, Kumar et al., EMNLP 2022, Yuqi Zhu

## Mars 7
- *Prompt Injection: Parameterization of Fixed Inputs*, Choi et al., ICLR 2023 **Withdraw**, Huangzhao Zhang
- *PromptAttack: Prompt-based Attack for Language Models via Gradient Search*, Shi et al., NLPCC 2022, Huangzhao zhang
- *Ignore Previous Prompt: Attack Techniques For Lanugage Models*, Parez et al., NIPS 2022, Huangzhao Zhang
- *Practical Program Repair in the Era of Large Pre-trained Language Models*, Xia et al., ICSE 2022, Yuwei Zhang

## Mars 14
- *Planning with Large Language Models for Code Generation*, Zhang et al., ICLR 2023, Zheng Fang
- *Automating Code-Related Tasks Through Transformers: The Impact of Pre-training*, Tufano et al., ICSE 2023, Yongmin Li

## Mars 21
- *DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models*, Gong et al., ICLR 2023, Yihong Dong
- *Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning*, Nashid et al., ICSE 2023, Mingyang Geng
- *Larger language models do in-context learning differently*, Wei et al., ?, Xue Jiang
- *Is ChatGPT a General-Purpose Natural Language Processing Task Solver?*, Qin et al., ?, Han Peng

## Mars 28
- 
- Symbolic Discovery of Optimization Algorithms 

## Avril 4
- *self-instruct: aligning language model with self generated instructions*
- *Alpaca: A Strong, Replicable instruction-Following Model*
- *Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%*
- *On the Feasibility of Specialized Ability Stealing for Large Language Code Models*
- *Large Langauge Model are ... Testers*
- *CodeGeeX*
## Avril 11
- *RepoCoder: Repository-Level Code Completion Through Iterative Retrieve and Generation*
- *A Watermark for Large Language Models*
- *Visual ChatGPT: Talking, Erawing and Editing with Visual Foundation Models*
- *HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face*
## Avril 18
- *teaching large language model to self-debug*
- *Developer-Intent Driven Code Comment Generation*
## Avril 25
- *REPLUG: Retrieval-Augmented Black-Box Langauge Models*, Shi et al., 
- *Large Lanugage Models are Di`verse Role-Players for Summarization Evaluation*, Wu et al., `


## Mai 9
- *Large Language Models Are Human-Level Prompt Engineers*
- *API-Bank: A Benchmark for Tool-Augmented LLMs*

## Mai 16
- *Language Moels Don't Always Say What They Think: ...*
- *Distilling Step-by-Step! Outperforming Larger Langauge Models with Liess Training Data and Smaller Model Sizes*
- *Generating Sequences by Learning to Self-Correct*

## Mai 23
- *Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation*
- *Examing vulnarable fix with Large language model*

## juin 6
- *Forward Gradient Descent*
    - *Gradients without Backpropagation*
    - *Scaling Forward Gradient With Local Losses*
- *Tree of thoughts: Deliberate Problem Solving with Large Langauge Models*
- *OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities*

## juin 13
- *Unleashing infinite-Length Input Capacity for Large-scale Language Models with Self-Contolled Memory System*
- *Is Model Attention Align with Human Attention*
- *Dropout Reduces Underfitting*
- *A Simple and Effective ?*